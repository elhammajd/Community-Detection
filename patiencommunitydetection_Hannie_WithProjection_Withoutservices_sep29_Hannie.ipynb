{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43b8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import cypher, Graph, NodeMatcher, RelationshipMatcher, Subgraph, Walkable, Path, Node\n",
    "import pandas as pd\n",
    "import igraph\n",
    "import numpy as np\n",
    "from dfply import *\n",
    "from IPython.display import display\n",
    "from sqlalchemy import create_engine, event\n",
    "from sqlalchemy.engine.url import URL\n",
    "import datetime\n",
    "import time\n",
    "from treelib import Node, Tree\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Trusted Connection to Named Instance\n",
    "conn_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=KVHSQLPC56;DATABASE=AHDA;Trusted_Connection=yes;'\n",
    "# Create a URL for the sqlalchemy engine to use\n",
    "conn_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": conn_str})\n",
    "# Create the connection\n",
    "engine = create_engine(conn_url)\n",
    "\n",
    "\n",
    "class sql:\n",
    "    \n",
    "    def __init__(self,cohort_id=0):\n",
    "        \n",
    "        # Trusted Connection to Named Instance\n",
    "        conn_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": conn_str})\n",
    "        # Create the connection\n",
    "        self.engine = create_engine(conn_url)\n",
    "        self.cohort_id = cohort_id\n",
    "        self.interactions = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def patients_projection2(self):\n",
    "        \n",
    "        svc=self.interactions.service_class_id\n",
    "        svc_unique = np.unique(svc)\n",
    "        map={}\n",
    "        for svc in svc_unique:\n",
    "            index=np.where(self.interactions.service_class_id==svc)[0]\n",
    "            patients=self.interactions.iloc[index].patient_id\n",
    "\n",
    "            patients_unique=np.unique(patients)\n",
    "    \n",
    "            for p1 in patients_unique:\n",
    "                for p2 in patients_unique:\n",
    "                    if p1<p2:\n",
    "                        if p1 in map:\n",
    "                            result=map.get(p1)\n",
    "                            if p2 in result:\n",
    "                                weight=result.get(p2)\n",
    "                                result[p2]=weight+1\n",
    "                            else:\n",
    "                                result[p2]=1\n",
    "\n",
    "                        else:\n",
    "                            result={}\n",
    "                            result[p2]=1\n",
    "                        map[p1]=result\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "\n",
    "        df3 = pd.DataFrame()\n",
    "        source=[]\n",
    "        target=[]\n",
    "        weight=[]\n",
    "\n",
    "        for key in map:\n",
    "            neighbor=map.get(key);\n",
    "            for n in neighbor:\n",
    "               \n",
    "                source.append(key)\n",
    "                target.append(n)\n",
    "                weight.append(neighbor.get(n))\n",
    "                \n",
    "                \n",
    "        df3= pd.DataFrame({'source': source, 'target': target,'weight':weight})\n",
    "                \n",
    "\n",
    "        #df3.insert(loc=0,\n",
    "                         #column='source',\n",
    "                             # value=source)\n",
    "       # df3.insert(loc=1,\n",
    "                        # column='target',\n",
    "                             # value=target)\n",
    "\n",
    "       # df3.insert(loc=2,\n",
    "                        # column='weight',\n",
    "                              #Svalue=weight)\n",
    "        return df3\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    def patients_projection44(self):\n",
    "        svc = self.interactions.service_class_id\n",
    "        svc_unique = np.unique(svc)\n",
    "        patient_map = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        for svc in svc_unique:\n",
    "            index = np.where(self.interactions.service_class_id == svc)[0]\n",
    "            patients = self.interactions.iloc[index].patient_id\n",
    "            patients_unique = np.unique(patients)\n",
    "\n",
    "            for p1, p2 in combinations(patients_unique, 2):\n",
    "                patient_map[p1][p2] += 1\n",
    "\n",
    "        source = []\n",
    "        target = []\n",
    "        weight = []\n",
    "\n",
    "        for p1, neighbors in patient_map.items():\n",
    "            for p2, count in neighbors.items():\n",
    "                source.append(p1)\n",
    "                target.append(p2)\n",
    "                weight.append(count)\n",
    "\n",
    "        df3 = pd.DataFrame({'source': source, 'target': target, 'weight': weight})\n",
    "        return df3\n",
    "    \n",
    "    \n",
    "    def patients_interactions(self):\n",
    "      \n",
    "        if self.cohort_id != -1:\n",
    "            query = \"\"\"SELECT i.patient_id,service_class_id,start_datetime  FROM  HF.interactions i\n",
    "                 INNER JOIN HF.patient_cohort_list pcl on pcl.patient_id = i.patient_id \n",
    "                 WHERE pcl.cohort_id =\"\"\"+str(self.cohort_id)\n",
    "        \n",
    "        if self.patient_id_query  != \"\":\n",
    "            query =  self.patient_id_query \n",
    "            \n",
    "        if len(self.patient_id_df)  > 0:\n",
    "            # patient_list = self.patient_id_df[\"patient_id\"].tolist()\n",
    "            patient_string = '\\',\\''.join(self.patient_id_df[\"patient_id\"])\n",
    "            patient_string = '\\'' + patient_string + '\\''\n",
    "            query = \"\"\"SELECT i.patient_id,service_class_id,start_datetime  FROM  HF.interactions i\n",
    "                 INNER JOIN HF.demographics d on d.patient_id = i.patient_id \n",
    "                 WHERE d.patient_id IN (\"\"\" + patient_string + \"\"\") \"\"\"\n",
    "            \"\"\"WITH FirstFall AS (\"\"\" + patient_string + \"\"\")\n",
    "                SELECT \n",
    "                       Distinct i.patient_id, i.service_class_id, i.start_datetime, i.end_datetime\n",
    "                FROM \n",
    "                       FirstFall\n",
    "                INNER JOIN \n",
    "                       HF.interactions i ON i.patient_id = FirstFall.patient_id\n",
    "                WHERE \n",
    "                       i.start_datetime < first_fall\n",
    "\n",
    "                AND \n",
    "                        i.service_class_id NOT IN (80,150,152,68)\n",
    "\n",
    "                ORDER BY i.patient_id, i.start_datetime\"\"\"\n",
    "            \n",
    "       # query += \"\"\" ORDER BY i.patient_id,start_datetime\"\"\"\n",
    "\n",
    "\n",
    "        df = pd.read_sql(query,engine)\n",
    "       \n",
    "        #interactions= pd.read_sql(query,engine)\n",
    "        # result=df.merge(patient_id_df, how='inner', on = 'patient_id')\n",
    "        # self.interactions=result[result['patient_id'].notna()]\n",
    "        # self.interactions=df\n",
    "        # return self.interactions\n",
    "        return df\n",
    "        \n",
    "                    \n",
    "    def sequence(self):\n",
    "      \n",
    "        \n",
    "       # query = '''SELECT service_class_id ,patient_id,start_datetime FROM  HF.interactions\n",
    "       #WHERE patient_id IN (SELECT patient_id FROM HF.patient_cohort_list WHERE cohort_id ='''+''+str(self.cohort_id)+\") ORDER BY start_datetime, patient_id \"\n",
    "        #  df=\n",
    "         #df = pd.read_sql(query,self.engine)\n",
    "        df=self.interactions\n",
    "        df = df.astype({\"service_class_id\": str}, errors='raise')\n",
    "        #df4=df.groupby(by='patient_id',as_index = False).agg({\n",
    "                                     # 'seq':lambda x: list(x)})\n",
    "            \n",
    "        df4=df.groupby(by='patient_id',as_index = False).agg({'service_class_id': ' '.join})\n",
    "                                     #'seq':lambda x: list(x)})    \n",
    "        # df4=pd.read_csv(\"H:/Projects/MSF_Covid/ErnieShare/DATA/coh04_schiz_pats.csv\")\n",
    "        return df4                    \n",
    "            \n",
    "            \n",
    "    def tf_idf_matrix(self):\n",
    " \n",
    "        corpus=self.sequence()\n",
    "       \n",
    "            \n",
    "       \n",
    "        \n",
    "        corpus.columns = ['pat','svc']\n",
    "\n",
    "        self.svcCodes = corpus[['svc']]\n",
    "        patHD = corpus[['pat']]\n",
    "        corpus_df = self.svcCodes[0:]\n",
    "        corpus_df = corpus_df.squeeze()\n",
    "        cv = CountVectorizer(token_pattern='\\\\b(\\\\w+)\\\\b')\n",
    "        cv_matrix = cv.fit_transform(corpus_df)\n",
    "        cv_matrix = cv_matrix.toarray()\n",
    "\n",
    "\n",
    "        # get all unique words in the corpus\n",
    "        vocab = cv.get_feature_names_out()\n",
    "        # show document feature vectors\n",
    "        df = pd.DataFrame(cv_matrix, columns=vocab)\n",
    "        self.svcINT = vocab.astype(int)\n",
    "        a=df.columns\n",
    "        self.service_class_id = pd.DataFrame(a.transpose().to_list(),columns=['service_class_id'])\n",
    "        dfpat = pd.concat([patHD,df],axis=1)\n",
    "        dfpat.iloc[[0]]\n",
    "        dfT = dfpat.transpose()\n",
    "        xyz = dfT.iloc[[0]].values[0].tolist()\n",
    "        dfT.columns = xyz\n",
    "        dfU = dfT[1:]\n",
    "        tv = TfidfTransformer()\n",
    "        \n",
    "\n",
    "        tv_matrix = tv.fit_transform(dfU)\n",
    "        tv_matrix = tv_matrix.toarray()\n",
    "        tvocab = tv.get_feature_names_out()\n",
    "     \n",
    "        rawTFIDF = pd.DataFrame(np.round(tv_matrix, 2), columns=patHD)\n",
    "        rawTFIDF = pd.concat([self.service_class_id,rawTFIDF],axis=1)\n",
    "        abcd = pd.DataFrame(np.round(tv_matrix, 2), columns=tvocab)\n",
    "        TF = rawTFIDF.copy(deep=True)\n",
    "\n",
    "        self.similarity_matrix = cosine_similarity(tv_matrix)\n",
    "        similarity_df = pd.DataFrame(self.similarity_matrix)\n",
    "        similarity_df.loc['Total'] = round(similarity_df.sum()/len(similarity_df)*100,2)\n",
    "        simCol = similarity_df.iloc[[-1][:]].transpose()\n",
    "        self.simCol = simCol.rename(columns={'Total':'Sim.%'})\n",
    "        return rawTFIDF\n",
    "\n",
    "\n",
    "    def numberofpatients(self,interaction,node):\n",
    "       \n",
    "        index=np.where(interaction.service_class_id==node)[0]\n",
    "        df=interaction.iloc[index]\n",
    "        \n",
    "        patients=df.patient_id.unique()                     \n",
    "        numberofP=len(patients)\n",
    "        return numberofP\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "    def normalized_weight_service_projection(self):\n",
    "        df=self.interactions\n",
    "       \n",
    "        result1=self.service_class_projection() \n",
    "        \n",
    "        \n",
    "        \n",
    "        df=df[['service_class_id','patient_id']]\n",
    "        df=df.drop_duplicates()\n",
    "        df2 = df.groupby(['service_class_id'],as_index=False).count()\n",
    "        df2=df2.rename(columns={'service_class_id': 'service_class_id_x','patient_id': 'coun1t'})\n",
    "        df3=df2.rename(columns={'service_class_id_x': 'service_class_id_y','patient_id': 'count2'})\n",
    "        result1=result1.rename(columns={'count': 'weight'})\n",
    "        x_count=pd.merge(result1, df2, how=\"left\", on='service_class_id_x')\n",
    "        x_y_count=pd.merge(x_count, df3, how=\"left\", on='service_class_id_y')\n",
    "        y=x_y_count\n",
    "        y['normalized_weight']=y['weight']/((y['coun1t_x']+y['coun1t_y'])-y['weight'])\n",
    "        #y=y[['service_class_id_x','service_class_id_y','normalized_weight']]\n",
    "        display(y)\n",
    "        return y\n",
    "                                                                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def1f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class communityDetection:\n",
    "    \n",
    "    def __init__(self,cohort_id=-1, patient_id_df='',patient_id_query='',interactions=''):\n",
    "            '''self.patient_id_df=patient_id_df\n",
    "            self.cohort_id=cohort_id\n",
    "            self.patient_id_query=patient_id_query\n",
    "            self.sql=sql(cohort_id=self.cohort_id)\n",
    "            self.sql.patient_id_df=patient_id_df\n",
    "            self.sql.cohort_id=cohort_id\n",
    "            self.sql.patient_id_query=patient_id_query\n",
    "            \n",
    "            self.sql.interactions = self.sql.patients_interactions()'''\n",
    "            self.sql=sql()\n",
    "            self.sql.interactions=interactions\n",
    "           \n",
    "            \n",
    "    def takeFromNeo(self):\n",
    "        g =Neo4j('bolt://localhost:7687', 'neo4j', 'New_Neo4j')\n",
    "        df=pd.read_csv('cohort52new.csv')\n",
    "        self.patients=g.patients()\n",
    "        display(self.patients)\n",
    "        #df=graph.query(b=self.cohort_id)\n",
    "        edges, weights = [], []\n",
    "        # Iterate over each row in the csv using reader object\n",
    "        for i in range(len(df)):\n",
    "            row=df.iloc[i]\n",
    "            u, v, weight = row[0],row[1],row[2]\n",
    "            \n",
    "            edges.append((u,v))\n",
    "            weights.append(float(weight))\n",
    "        self.graph = igraph.Graph(edges, edge_attrs={\"weight\": weights})\n",
    "         #self.label=graph.label()\n",
    "        \n",
    "       # self.label=self.label.iloc[0:253]\n",
    "         #self.label= pd.read_csv('labelnew2.csv')\n",
    "         #return self.patients\n",
    "        #display(self.label)\n",
    "        #creating adjacency list of the graph\n",
    "\n",
    "    def create_graph_for_patients2(self):\n",
    "               \n",
    "        df=self.sql.patients_projection2()\n",
    "        print(\"start\")\n",
    "        \n",
    "\n",
    "        #df=graph.query(b=self.cohort_id)\n",
    "        edges, weights = [], []\n",
    "        # Iterate over each row in the csv using reader object\n",
    "        self.graph2= igraph.Graph.TupleList(df.itertuples(index=False), directed=False, weights=False, edge_attrs=\"weight\")\n",
    "\n",
    "\n",
    "    def create_graph_for_patients(self):\n",
    "        \n",
    "        \n",
    "        #self.patients=self.sql.patients()\n",
    "        \n",
    "        \n",
    "       # df=self.sql.patinet_projection()\n",
    "        df=self.sql.patients_projection2()\n",
    "        #df=self.sql.patients_projection2()\n",
    "        \n",
    "        map1={}   \n",
    "        map2={}\n",
    "        l1=np.unique(df.target)\n",
    "        l2=np.unique(df.source)\n",
    "        list1 = l1.tolist()\n",
    "        list2 = l2.tolist()\n",
    "        list3=list1+list2\n",
    "        arr = np.array(list3)\n",
    "        unique_patients=np.unique(arr)\n",
    "        len(unique_patients)\n",
    "\n",
    "        #for mappping\n",
    "        i=0\n",
    "        while i <len(unique_patients):\n",
    "            vertex=unique_patients[i]\n",
    "            map1[vertex]=i\n",
    "            map2[i]=vertex\n",
    "            i=i+1\n",
    "\n",
    "        #transfer a df of strings to dataframe of integers with using map\n",
    "        source=[]\n",
    "        target=[]\n",
    "        weight=[]\n",
    "        for i in range(len(df)):\n",
    "            row=df.iloc[i]\n",
    "            u, v, weight2 = row[0],row[1],row[2]\n",
    "\n",
    "            source.append(map1.get(u))\n",
    "            target.append(map1.get(v))\n",
    "            weight.append(weight2)\n",
    "\n",
    "        df2= pd.DataFrame()\n",
    "\n",
    "        df2.insert(loc=0,\n",
    "                     column='source',\n",
    "                 value=source)\n",
    "\n",
    "        df2.insert(loc=1,\n",
    "                         column='target',\n",
    "                    value=target)\n",
    "        df2.insert(loc=2,\n",
    "                         column='weight',\n",
    "                    value=weight)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        print(\"done!\")\n",
    "        #df=graph.query(b=self.cohort_id)\n",
    "        edges, weights = [], []\n",
    "        # Iterate over each row in the csv using reader object\n",
    "        for i in range(len(df)):\n",
    "            row=df2.iloc[i]\n",
    "            u, v, weight = row[0],row[1],row[2]\n",
    "            \n",
    "            edges.append((u,v))\n",
    "            weights.append(float(weight))\n",
    "        self.graph2 = igraph.Graph(edges, edge_attrs={\"weight\": weights})\n",
    "        self.map3=map1\n",
    "        self.map4=map2\n",
    "        \n",
    "        \n",
    "    \n",
    "    def community_detection_on_patinets(self, numberofiterations=-1):\n",
    "        \n",
    "        numberofitterations=numberofiterations\n",
    "        # first itteration\n",
    "        louvain = self.graph2.community_multilevel(weights=self.graph2.es['weight'], return_levels=False)\n",
    "        pi = []\n",
    "        piI = []\n",
    "\n",
    "        numberofclustures = 0\n",
    "        for i in range(len(louvain)):\n",
    "            component = louvain[i]\n",
    "\n",
    "            if (len(component) > 1):\n",
    "                piI.append(component)\n",
    "\n",
    "        pi.append(piI)\n",
    "        itteration = 1\n",
    "        if numberofitterations==-1:\n",
    "            while True:\n",
    "                piI1 = []\n",
    "                Previous_components = pi[itteration - 1]\n",
    "                for community in pi[itteration - 1]:\n",
    "\n",
    "                    # print(community)\n",
    "                    induced_subgraph = self.graph2.induced_subgraph(list(community))\n",
    "                    louvain1 = induced_subgraph.community_multilevel(weights=induced_subgraph.es['weight'], return_levels=False)\n",
    "                    for i in range(len(louvain1)):\n",
    "                        subgraph = louvain1[i]\n",
    "                        # print(subgraph)\n",
    "                        lst = []\n",
    "                        for maped_vertex in subgraph:\n",
    "                            map = community[maped_vertex]\n",
    "                            lst.append(map)\n",
    "                        # print(lst)\n",
    "                        # print(tuple(lst))\n",
    "\n",
    "                        piI1.append((lst))\n",
    "                pi.append(piI1)\n",
    "                current_components = pi[itteration]\n",
    "                if not current_components > Previous_components:\n",
    "                    break\n",
    "                else:\n",
    "                    itteration += 1\n",
    "        if numberofitterations!=-1:\n",
    "            if numberofitterations == -1:\n",
    "                while True:\n",
    "                    piI1 = []\n",
    "                    Previous_components = pi[itteration - 1]\n",
    "                    for community in pi[itteration - 1]:\n",
    "\n",
    "                        # print(community)\n",
    "                        induced_subgraph = self.graph2.induced_subgraph(list(community))\n",
    "                        louvain1 = induced_subgraph.community_multilevel(weights=induced_subgraph.es['weight'],\n",
    "                                                                         return_levels=False)\n",
    "                        for i in range(len(louvain1)):\n",
    "                            subgraph = louvain1[i]\n",
    "                            # print(subgraph)\n",
    "                            lst = []\n",
    "                            for maped_vertex in subgraph:\n",
    "                                map = community[maped_vertex]\n",
    "                                lst.append(map)\n",
    "                            # print(lst)\n",
    "                            # print(tuple(lst))\n",
    "\n",
    "                            piI1.append((lst))\n",
    "                    pi.append(piI1)\n",
    "                    current_components = pi[itteration]\n",
    "                    if not current_components > Previous_components:\n",
    "                        break\n",
    "                    else:\n",
    "                        itteration += 1\n",
    "\n",
    "\n",
    "            if numberofitterations != -1:\n",
    "                while itteration<numberofitterations:\n",
    "                    piI1 = []\n",
    "                    Previous_components = pi[itteration - 1]\n",
    "                    for community in pi[itteration - 1]:\n",
    "\n",
    "                        # print(community)\n",
    "                        induced_subgraph = self.graph2.induced_subgraph(list(community))\n",
    "                        louvain1 = induced_subgraph.community_multilevel(weights=induced_subgraph.es['weight'],\n",
    "                                                                         return_levels=False)\n",
    "                        for i in range(len(louvain1)):\n",
    "                            subgraph = louvain1[i]\n",
    "                            # print(subgraph)\n",
    "                            lst = []\n",
    "                            for maped_vertex in subgraph:\n",
    "                                map = community[maped_vertex]\n",
    "                                lst.append(map)\n",
    "                            # print(lst)\n",
    "                            # print(tuple(lst))\n",
    "\n",
    "                            piI1.append((lst))\n",
    "                    pi.append(piI1)\n",
    "                    current_components = pi[itteration]\n",
    "                    if not current_components > Previous_components:\n",
    "                        break\n",
    "                    else:\n",
    "                        itteration += 1\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"number of all iterations\",len(pi))\n",
    "        for i in range(len(pi)):\n",
    "            iteration=pi[i]\n",
    "            print(\"number of commmunities in \"+str(i+1)+' '+\"iteration\",len(iteration ))\n",
    "      \n",
    "    \n",
    "       #prevent calling community detection for patients in create table function:\n",
    "       #return pi,map1,map2\n",
    "        self.pi6=pi\n",
    "        return self.pi6,self.graph2\n",
    "           \n",
    "    \n",
    "    def create_table_for_patients(self,iteration2=-1):\n",
    "        itteration2=iteration2\n",
    "        if itteration2==-1:\n",
    "            itteration=len(self.pi6)\n",
    "            print(itteration)\n",
    "        if itteration2 !=-1:\n",
    "            itteration=itteration2\n",
    "        weight = []\n",
    "        out_degree=[]\n",
    "        weightd_degree=[]\n",
    "        for j in range(len(self.pi6)):\n",
    "            itteration1 = self.pi6[j]\n",
    "\n",
    "            weightI = []\n",
    "            out_degreeI = list()\n",
    "            weightd_degreeI=[]\n",
    "            for i in range(len(itteration1)):\n",
    "                component = itteration1[i]\n",
    "\n",
    "                subgraph = self.graph2.induced_subgraph(component)\n",
    "                total_degree = self.graph2.strength(component, mode='all', loops=True, weights=\"weight\")\n",
    "                weightd_degreeI.append(total_degree)\n",
    "                lst = list(range(len(component)))\n",
    "                in_degree = subgraph.strength(lst, mode='all', loops=True, weights=\"weight\")\n",
    "                weightI.append(in_degree)\n",
    "                lst2=[]\n",
    "                for item1, item2 in zip(total_degree,in_degree ):\n",
    "                    item = item1 - item2\n",
    "                    lst2.append(item)\n",
    "                out_degreeI.append(lst2) \n",
    "\n",
    "            weight.append(weightI)\n",
    "            out_degree.append(out_degreeI)\n",
    "            weightd_degree.append(weightd_degreeI)\n",
    "\n",
    "\n",
    "        if itteration2==-1:\n",
    "            itteration=len(self.pi6)\n",
    "            #for community id\n",
    "            \n",
    "            self.vertices = []\n",
    "            idd = 1\n",
    "            id = []\n",
    "            age=[]\n",
    "            gender=[]\n",
    "            primary=[]\n",
    "            fsa=[]\n",
    "            n_encounters=[]\n",
    "            n_interactions=[]\n",
    "            first_admit_dt=[]\n",
    "            last_admit_dt=[]\n",
    "            covid_positive=[]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for component in self.pi6[itteration - 1]:\n",
    "                for vertex in component:\n",
    "                    v= self.map4.get(vertex)\n",
    "                    self.vertices.append(v)\n",
    "                    id.append(idd)\n",
    "                idd = idd + 1\n",
    "                    \n",
    "                \n",
    "                   \n",
    "                   #f b=label.name[index]\n",
    "\n",
    "\n",
    "            #for weight\n",
    "            weightt=[]\n",
    "            for component in weight[itteration - 1]:\n",
    "                for degree in component:\n",
    "                    weightt.append(degree)\n",
    "\n",
    "\n",
    "\n",
    "            #for out degree       \n",
    "            out_degree2=[]   \n",
    "            for component in out_degree[itteration - 1]:\n",
    "                  for out_deg in component:\n",
    "                        out_degree2.append(out_deg)\n",
    "\n",
    "           #for weighted degree       \n",
    "            weighted_degree2=[]   \n",
    "            for component in weightd_degree[itteration - 1]:\n",
    "                  for weight_deg in component:\n",
    "                        weighted_degree2.append(weight_deg)             \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "            df.insert(loc=0,\n",
    "                      column='patients_id',\n",
    "                      value=self.vertices)\n",
    "\n",
    "            \n",
    "                   \n",
    "            \n",
    "            df.insert(loc=1,\n",
    "                      column='community_id',\n",
    "                      value=id)\n",
    "\n",
    "            df.insert(loc=2,\n",
    "                      column='in_degree',\n",
    "                      value=weightt)\n",
    "            df.insert(loc=3,\n",
    "                        column='out_degree',\n",
    "                        value=out_degree2)\n",
    "            \n",
    "\n",
    "\n",
    "            df.insert(loc=4,\n",
    "                        column='weighted_degree',\n",
    "                       value=weighted_degree2)\n",
    "            \n",
    "            \n",
    "\n",
    "     \n",
    "            return(df)\n",
    "\n",
    "        if itteration2!=-1:\n",
    "            #for id\n",
    "            # idd = 1\n",
    "            # id = []\n",
    "            # for component in self.pi6[itteration - 1]:\n",
    "                # for vertex in component:\n",
    "                    # id.append(idd)\n",
    "                # idd = idd + 1\n",
    "                \n",
    "            self.vertices = []\n",
    "            idd = 1\n",
    "            id = []\n",
    "            age=[]\n",
    "            gender=[]\n",
    "            primary=[]\n",
    "            fsa=[]\n",
    "            n_encounters=[]\n",
    "            n_interactions=[]\n",
    "            first_admit_dt=[]\n",
    "            last_admit_dt=[]\n",
    "            covid_positive=[]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for component in self.pi6[itteration - 1]:\n",
    "                for vertex in component:\n",
    "                    v= self.map4.get(vertex)\n",
    "                    self.vertices.append(v)\n",
    "                    '''index = np.where(self.patients.patient_id== v)[0]'''\n",
    "                    id.append(idd)\n",
    "                idd = idd + 1 \n",
    "                  \n",
    "\n",
    "            #for weight\n",
    "            weightt=[]\n",
    "            for component in weight[itteration - 1]:\n",
    "                for degree in component:\n",
    "                    weightt.append(degree)\n",
    "            df = pd.DataFrame()                   \n",
    "                    \n",
    "                    \n",
    "\n",
    "            #for out degree       \n",
    "            out_degree2=[]   \n",
    "            for component in out_degree[itteration - 1]:\n",
    "                  for out_deg in component:\n",
    "                        out_degree2.append(out_deg)\n",
    "\n",
    "            #for weighted degree       \n",
    "            weighted_degree2=[]   \n",
    "            for component in weightd_degree[itteration - 1]:\n",
    "                 for weight_deg in component:\n",
    "                        weighted_degree2.append(weight_deg)            \n",
    "\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "            df.insert(loc=0,\n",
    "                      column='patients_id',\n",
    "                      value=self.vertices)\n",
    "\n",
    "            \n",
    "            df.insert(loc=1,\n",
    "                      column='community_id',\n",
    "                      value=id)\n",
    "\n",
    "            df.insert(loc=2,\n",
    "                      column='in_degree',\n",
    "                      value=weightt)\n",
    "            df.insert(loc=3,\n",
    "                        column='out_degree',\n",
    "                        value=out_degree2)\n",
    "            \n",
    "\n",
    "\n",
    "            df.insert(loc=4,\n",
    "                        column='weighted_degree',\n",
    "                       value=weighted_degree2)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            return(df)\n",
    "        \n",
    "\n",
    "    def database(self):\n",
    "        \n",
    "        #self.interactions2=self.sql.interaction()\n",
    "        \n",
    "        self.allPatients=self.sql.patients()\n",
    "        #self.interactions2['counter']=1\n",
    "\n",
    "    \n",
    "    def create_table_for_patients3(self,iteration2=-1):\n",
    "        itteration2=iteration2\n",
    "        if itteration2==-1:\n",
    "            itteration=len(self.pi6)\n",
    "            print(itteration)\n",
    "        if itteration2 !=-1:\n",
    "            itteration=itteration2\n",
    "  \n",
    "        itteration1=self.pi6[itteration-1]\n",
    "        weightI = []\n",
    "        out_degreeI = list()\n",
    "        weightd_degreeI=[]\n",
    "        for i in range(len(itteration1)):\n",
    "            component = itteration1[i]\n",
    "\n",
    "            subgraph = self.graph2.induced_subgraph(component)\n",
    "            total_degree = self.graph2.strength(component, mode='all', loops=True, weights=\"weight\")\n",
    "            weightd_degreeI.append(total_degree)\n",
    "            lst = list(range(len(component)))\n",
    "            in_degree = subgraph.strength(lst, mode='all', loops=True, weights=\"weight\")\n",
    "            weightI.append(in_degree)\n",
    "            lst2=[]\n",
    "            for item1, item2 in zip(total_degree,in_degree ):\n",
    "                item = item1 - item2\n",
    "                lst2.append(item)\n",
    "            out_degreeI.append(lst2) \n",
    "\n",
    "\n",
    "\n",
    "        if itteration2==-1:\n",
    "            itteration=len(self.pi6)\n",
    "            #for community id\n",
    "            \n",
    "            self.vertices = []\n",
    "            idd = 1\n",
    "            id = []\n",
    "            age=[]\n",
    "            gender=[]\n",
    "            primary=[]\n",
    "            fsa=[]\n",
    "            n_encounters=[]\n",
    "            n_interactions=[]\n",
    "            first_admit_dt=[]\n",
    "            last_admit_dt=[]\n",
    "            covid_positive=[]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for component in self.pi6[itteration - 1]:\n",
    "                for vertex in component:\n",
    "                    v= self.map4.get(vertex)\n",
    "                    self.vertices.append(v)\n",
    "                    id.append(idd)\n",
    "                idd = idd + 1\n",
    "                    \n",
    "                \n",
    "                   \n",
    "                   #f b=label.name[index]\n",
    "\n",
    "\n",
    "            #for weight\n",
    "            weightt=[]\n",
    "            for component in weightI:\n",
    "                for degree in component:\n",
    "                    weightt.append(degree)\n",
    "\n",
    "\n",
    "\n",
    "            #for out degree       \n",
    "            out_degree2=[]   \n",
    "            for component in out_degreeI:\n",
    "                  for out_deg in component:\n",
    "                        out_degree2.append(out_deg)\n",
    "\n",
    "           #for weighted degree       \n",
    "            weighted_degree2=[]   \n",
    "            for component in weightd_degreeI:\n",
    "                  for weight_deg in component:\n",
    "                        weighted_degree2.append(weight_deg)             \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "            df.insert(loc=0,\n",
    "                      column='patients_id',\n",
    "                      value=self.vertices)\n",
    "\n",
    "            \n",
    "                   \n",
    "            \n",
    "            df.insert(loc=1,\n",
    "                      column='community_id',\n",
    "                      value=id)\n",
    "\n",
    "            df.insert(loc=2,\n",
    "                      column='in_degree',\n",
    "                      value=weightt)\n",
    "            df.insert(loc=3,\n",
    "                        column='out_degree',\n",
    "                        value=out_degree2)\n",
    "            \n",
    "\n",
    "\n",
    "            df.insert(loc=4,\n",
    "                        column='weighted_degree',\n",
    "                       value=weighted_degree2)\n",
    "            \n",
    "            \n",
    "\n",
    "     \n",
    "            return(df)\n",
    "\n",
    "        if itteration2!=-1:\n",
    "            #for id\n",
    "            # idd = 1\n",
    "            # id = []\n",
    "            # for component in self.pi6[itteration - 1]:\n",
    "                # for vertex in component:\n",
    "                    # id.append(idd)\n",
    "                # idd = idd + 1\n",
    "                \n",
    "            self.vertices = []\n",
    "            idd = 1\n",
    "            id = []\n",
    "            age=[]\n",
    "            gender=[]\n",
    "            primary=[]\n",
    "            fsa=[]\n",
    "            n_encounters=[]\n",
    "            n_interactions=[]\n",
    "            first_admit_dt=[]\n",
    "            last_admit_dt=[]\n",
    "            covid_positive=[]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for component in self.pi6[itteration - 1]:\n",
    "                for vertex in component:\n",
    "                    \n",
    "                    self.vertices.append(self.graph2.vs[vertex][\"name\"])\n",
    "                    '''index = np.where(self.patients.patient_id== v)[0]'''\n",
    "                    id.append(idd)\n",
    "                idd = idd + 1 \n",
    "\n",
    "             \n",
    "\n",
    "            #for weight\n",
    "            weightt=[]\n",
    "            for component in weightI:\n",
    "                for degree in component:\n",
    "                    weightt.append(degree)\n",
    "            df = pd.DataFrame()                   \n",
    "                    \n",
    "                    \n",
    "\n",
    "            #for out degree       \n",
    "            out_degree2=[]   \n",
    "            for component in out_degreeI:\n",
    "                  for out_deg in component:\n",
    "                        out_degree2.append(out_deg)\n",
    "\n",
    "            #for weighted degree       \n",
    "            weighted_degree2=[]   \n",
    "            for component in weightd_degreeI:\n",
    "                 for weight_deg in component:\n",
    "                        weighted_degree2.append(weight_deg)            \n",
    "\n",
    "                        \n",
    "                        \n",
    "\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "            df.insert(loc=0,\n",
    "                      column='patients_id',\n",
    "                      value=self.vertices)\n",
    "\n",
    "            \n",
    "            df.insert(loc=1,\n",
    "                      column='community_id',\n",
    "                      value=id)\n",
    "\n",
    "            df.insert(loc=2,\n",
    "                      column='in_degree',\n",
    "                      value=weightt)\n",
    "            df.insert(loc=3,\n",
    "                        column='out_degree',\n",
    "                        value=out_degree2)\n",
    "            \n",
    "\n",
    "\n",
    "            df.insert(loc=4,\n",
    "                        column='weighted_degree',\n",
    "                       value=weighted_degree2)\n",
    "          \n",
    "            return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64734319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientCommunityDetectionAndAnalysis:\n",
    "    def __init__(self, cohort_id=-1, patient_id_df='', patient_id_query='', interactions='', data='HF'):\n",
    "        self.community_detector = communityDetection(cohort_id, patient_id_df, patient_id_query, interactions)\n",
    "        self.sql = sql()\n",
    "        self.HF = True\n",
    "        self.cohort = cohort_id\n",
    "\n",
    "\n",
    "        if patient_id_df:\n",
    "            patient_ids_df = pd.read_csv(patient_id_df)\n",
    "            patient_ids = patient_ids_df['patient_id'].astype(str).tolist()\n",
    "            interactions = find_cohort_new_interactions(cohort_id, patient_ids=patient_ids)\n",
    "        else:\n",
    "            interactions = find_cohort_new_interactions(cohort_id)\n",
    "\n",
    "            \n",
    "\n",
    "        self.Hf = data != 'HF'\n",
    "\n",
    "        print(f\"Number of unique patients in interactions: {len(interactions.patient_id.unique())}\")\n",
    "\n",
    "        if len(interactions.patient_id.unique()) >= 15900:\n",
    "            print(\"We select 15,000 sample patients for the rest of the analysis\")\n",
    "            df = pd.DataFrame(interactions.patient_id.unique(), columns=['patient_id'])\n",
    "            df = df.sample(n=15492)\n",
    "            interactions = interactions[interactions['patient_id'].isin(df['patient_id'])]\n",
    "            result = interactions\n",
    "        else:\n",
    "            result = interactions\n",
    "\n",
    "        print(\"df\")\n",
    "        print(result)\n",
    "        c = communityDetection(cohort_id=-1, patient_id_df='', patient_id_query='', interactions=result)\n",
    "        c.create_graph_for_patients2()\n",
    "        a, b = c.community_detection_on_patinets()\n",
    "        self.df1 = c.create_table_for_patients3(iteration2=1)\n",
    "        self.df2 = c.create_table_for_patients3(iteration2=2)\n",
    "        \n",
    "        # Assign the populated communityDetection instance if needed\n",
    "        self.community_detector = c\n",
    "        \n",
    "        \n",
    "    def interactionsAndPatientsOFeachSvc_HF( self,community_df):\n",
    "        patient_string = '\\',\\''.join(community_df[\"patients_id\"])\n",
    "        patient_string = '\\'' + patient_string + '\\''\n",
    "\n",
    "        query= \"\"\"SELECT service_class_id,count(distinct patient_id ) as patients\n",
    "          FROM HF.interactions \n",
    "          WHERE patient_id IN (\n",
    "            \"\"\" + patient_string + \"\"\")\"\"\"+\"\"\" and service_class_id!=-1\n",
    "            group by service_class_id\n",
    "            order by patients DESC \"\"\"\n",
    "\n",
    "        return  pd.read_sql(query,engine)\n",
    "\n",
    "    def creat_patient_svc_interaction_forCommunities(self,iteration=1):\n",
    "        \n",
    "        if iteration==1:\n",
    "            df=self.df1\n",
    "        if iteration==2:\n",
    "            df=self.df2\n",
    "        if self.HF==True:    \n",
    "            take_cohort_svcid_label=self.take_cohort_svcid_labell_HF()\n",
    "        else:\n",
    "            take_cohort_svcid_label=self.take_cohort_svcid_labell_Core(self.cohort)\n",
    "            \n",
    "        i=0\n",
    "        for community in df.community_id.unique():\n",
    "            index=np.where(df.community_id==community)\n",
    "            community_df=df.iloc[index]\n",
    "            if self.HF==True:\n",
    "                interactionsAndPatientsOFeachSvc_df= self.interactionsAndPatientsOFeachSvc_HF(community_df)\n",
    "            else:\n",
    "                interactionsAndPatientsOFeachSvc_df= self.interactionsAndPatientsOFeachSvc_Core(community_df)\n",
    "                \n",
    "                \n",
    "            interactionsAndPatientsOFeachSvcMergeWithAllSvc= pd.merge(take_cohort_svcid_label, interactionsAndPatientsOFeachSvc_df, on='service_class_id', how='left')\n",
    "            new_col_name = str(community)+ '  #patients'\n",
    "            new_col_name2 = str(community)+ '  #interactions' \n",
    "            col_dict = {'patients':new_col_name }\n",
    "            interactionsAndPatientsOFeachSvcMergeWithAllSvc=interactionsAndPatientsOFeachSvcMergeWithAllSvc.rename(columns=col_dict)\n",
    "            interactionsAndPatientsOFeachSvcMergeWithAllSvc=interactionsAndPatientsOFeachSvcMergeWithAllSvc.fillna(0)\n",
    "            #calculate the cohort%\n",
    "            summury=interactionsAndPatientsOFeachSvcMergeWithAllSvc\n",
    "            \n",
    "             #summury['interaction/p'+ str(community)] = summury.apply(lambda row: safe_division(row.iloc[2], row.iloc[1]), axis=1)\n",
    "            summury['community%'+ str(community)] =  summury.apply(lambda row: self.safe_division2(row.iloc[1], len(community_df)), axis=1)\n",
    "\n",
    "            if(i!=0):\n",
    "                df_copy=pd.merge( df_copy,  summury, on='service_class_id', how='left')\n",
    "\n",
    "            if(i==0):  \n",
    "                df_copy =  summury.copy(deep=True)\n",
    "            i=i+1\n",
    "        df_copy = df_copy.fillna(0)     \n",
    "        return  df_copy\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1edf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import cypher, Graph, NodeMatcher, RelationshipMatcher, Subgraph, Walkable, Path, Node\n",
    "import pandas as pd\n",
    "import igraph\n",
    "import numpy as np\n",
    "from dfply import *\n",
    "from IPython.display import display\n",
    "from sqlalchemy import create_engine, event\n",
    "from sqlalchemy.engine.url import URL\n",
    "import datetime\n",
    "from treelib import Node, Tree\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Trusted Connection to Named Instance\n",
    "conn_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=KVHSQLPC56;DATABASE=AHDA;Trusted_Connection=yes;'\n",
    "# Create a URL for the sqlalchemy engine to use\n",
    "conn_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": conn_str})\n",
    "# Create the connection\n",
    "engine = create_engine(conn_url)\n",
    "\n",
    "def return_interaction_hf(df):\n",
    "    patient_string = '\\',\\''.join(df[\"patient_id\"])\n",
    "    patient_string = '\\'' + patient_string + '\\''\n",
    "    query =  \"\"\"WITH FirstFall AS (\"\"\" + patient_string + \"\"\")\n",
    "SELECT \n",
    "       i.patient_id, i.service_class_id, i.start_datetime, i.end_datetime\n",
    "FROM \n",
    "       FirstFall\n",
    "INNER JOIN \n",
    "       HF.interactions i ON i.patient_id = FirstFall.patient_id\n",
    "WHERE \n",
    "       i.start_datetime < first_fall\n",
    "AND        \n",
    "        i.service_class_id NOT IN (80,150,152,68)\n",
    "ORDER BY i.patient_id, i.start_datetime\"\"\"\n",
    "    result=pd.read_sql(query,engine)\n",
    "    return result\n",
    "\n",
    "def find_cohort_new_interactions(cohort_id, patient_ids=None):\n",
    "    if patient_ids is not None:\n",
    "        # Construct a proper CTE with SELECT statement and column alias\n",
    "        #patient_ids_str = \"', '\".join([str(pid) for pid in patient_ids])\n",
    "        \n",
    "        query =  \"\"\"WITH FirstFall AS (\n",
    "       SELECT \n",
    "              Distinct patient_id\n",
    "              ,min([admit_datetime]) as first_fall\n",
    "              ,min(diagnosis_age) as first_fall_age\n",
    "       FROM \n",
    "              Core.acute_care_diagnoses acd\n",
    "       WHERE \n",
    "              diagnosis_type_code NOT IN ('3', '4', '0')\n",
    "              AND SUBSTRING(icd10_code, 1, 3) IN ('W06', 'W07', 'W08', 'W13', 'W14', 'W15', 'W16', 'W17', \n",
    "                                      'X80', 'Y01', 'Y30', 'W00', 'W01', 'W03', 'W04', 'W18', \n",
    "                                      'W10', 'W05')\n",
    "              AND diagnosis_age >= 65\n",
    "       GROUP BY \n",
    "              patient_id\n",
    ")\n",
    "SELECT \n",
    "       i.patient_id, i.service_class_id, i.start_datetime, i.end_datetime\n",
    "FROM \n",
    "       FirstFall\n",
    "INNER JOIN \n",
    "       HF.interactions i ON i.patient_id = FirstFall.patient_id\n",
    "WHERE \n",
    "       i.start_datetime < first_fall\n",
    "AND        \n",
    "        i.service_class_id NOT IN (80,150,152,68)\n",
    "ORDER BY i.patient_id, i.start_datetime\"\"\"\n",
    "\n",
    "    elif cohort_id:\n",
    "        query = \"\"\"  \n",
    "        SELECT  patient_id, service_class_id  \n",
    "        FROM HF.interactions \n",
    "        WHERE patient_id IN (\n",
    "            SELECT patient_id \n",
    "            FROM HF.patient_cohort_list \n",
    "            WHERE cohort_Id = {}\n",
    "        )\"\"\".format(cohort_id)\n",
    "    else:\n",
    "        query = \"\"\"  \n",
    "            WITH FirstFall AS (\n",
    "       SELECT \n",
    "              Distinct patient_id\n",
    "              ,min([admit_datetime]) as first_fall\n",
    "              ,min(diagnosis_age) as first_fall_age\n",
    "       FROM \n",
    "              Core.acute_care_diagnoses acd\n",
    "       WHERE \n",
    "              diagnosis_type_code NOT IN ('3', '4', '0')\n",
    "              AND SUBSTRING(icd10_code, 1, 3) IN ('W06', 'W07', 'W08', 'W13', 'W14', 'W15', 'W16', 'W17', \n",
    "                                      'X80', 'Y01', 'Y30', 'W00', 'W01', 'W03', 'W04', 'W18', \n",
    "                                      'W10', 'W05')\n",
    "              AND diagnosis_age >= 65\n",
    "       GROUP BY \n",
    "              patient_id\n",
    ")\n",
    "SELECT \n",
    "       i.patient_id, i.service_class_id, i.start_datetime, i.end_datetime\n",
    "FROM \n",
    "       FirstFall\n",
    "INNER JOIN \n",
    "       HF.interactions i ON i.patient_id = FirstFall.patient_id\n",
    "WHERE \n",
    "       i.start_datetime < first_fall\n",
    "AND        \n",
    "        i.service_class_id NOT IN (80,150,152,68)\n",
    "ORDER BY i.patient_id, i.start_datetime \n",
    "        \"\"\"  # No filtering by cohort_id\n",
    "\n",
    "    # Execute the query using the engine and return the result\n",
    "    result = pd.read_sql(query, engine)\n",
    "    print(result)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0374cdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       patient_id  service_class_id      start_datetime        end_datetime\n",
      "0        10001903               154 2016-06-07 12:43:00 2016-06-07 23:59:00\n",
      "1        10001903               151 2016-09-20 12:56:00 2016-09-20 23:59:00\n",
      "2        10001903                82 2016-11-24 08:00:00 2016-12-07 15:14:00\n",
      "3        10001903                59 2016-11-29 09:22:00 2017-04-07 10:20:00\n",
      "4        10001903               154 2016-12-16 11:13:00 2016-12-16 23:59:00\n",
      "...           ...               ...                 ...                 ...\n",
      "144085   CCCA0484               146 2019-07-19 06:38:00 2019-07-25 14:08:00\n",
      "144086   CCCA0484               138 2019-07-27 15:39:00 2019-10-14 13:53:00\n",
      "144087   CCCA0484               154 2019-07-30 14:17:00 2019-07-30 23:59:00\n",
      "144088   CCCA0484               154 2019-07-31 15:20:00 2019-07-31 23:59:00\n",
      "144089   CCCA0484               154 2019-08-08 13:52:00 2019-08-08 23:59:00\n",
      "\n",
      "[144090 rows x 4 columns]\n",
      "Number of unique patients in interactions: 13388\n",
      "df\n",
      "       patient_id  service_class_id      start_datetime        end_datetime\n",
      "0        10001903               154 2016-06-07 12:43:00 2016-06-07 23:59:00\n",
      "1        10001903               151 2016-09-20 12:56:00 2016-09-20 23:59:00\n",
      "2        10001903                82 2016-11-24 08:00:00 2016-12-07 15:14:00\n",
      "3        10001903                59 2016-11-29 09:22:00 2017-04-07 10:20:00\n",
      "4        10001903               154 2016-12-16 11:13:00 2016-12-16 23:59:00\n",
      "...           ...               ...                 ...                 ...\n",
      "144085   CCCA0484               146 2019-07-19 06:38:00 2019-07-25 14:08:00\n",
      "144086   CCCA0484               138 2019-07-27 15:39:00 2019-10-14 13:53:00\n",
      "144087   CCCA0484               154 2019-07-30 14:17:00 2019-07-30 23:59:00\n",
      "144088   CCCA0484               154 2019-07-31 15:20:00 2019-07-31 23:59:00\n",
      "144089   CCCA0484               154 2019-08-08 13:52:00 2019-08-08 23:59:00\n",
      "\n",
      "[144090 rows x 4 columns]\n",
      "start\n",
      "number of all iterations 10\n",
      "number of commmunities in 1 iteration 3\n",
      "number of commmunities in 2 iteration 12\n",
      "number of commmunities in 3 iteration 49\n",
      "number of commmunities in 4 iteration 178\n",
      "number of commmunities in 5 iteration 436\n",
      "number of commmunities in 6 iteration 691\n",
      "number of commmunities in 7 iteration 816\n",
      "number of commmunities in 8 iteration 835\n",
      "number of commmunities in 9 iteration 836\n",
      "number of commmunities in 10 iteration 836\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of PatientCommunityDetectionAndAnalysis\n",
    "a = PatientCommunityDetectionAndAnalysis(\n",
    "    cohort_id='',\n",
    "    #patient_id_df='',\n",
    "    patient_id_df='patient_ids_falls.csv',\n",
    "    patient_id_query='',  \n",
    "    interactions='',     \n",
    "    data='HF'         \n",
    ")\n",
    "\n",
    "# Access the community detection object\n",
    "community_detector = a.community_detector\n",
    "\n",
    "# Create the patient graph\n",
    "patient_community_df=community_detector.create_table_for_patients3(iteration2=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79e2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  patients_id  community_id  in_degree  out_degree  weighted_degree\n",
      "0   1005473B4             1    14235.0     12203.0          26438.0\n",
      "1    10255893             1    12080.0     10595.0          22675.0\n",
      "2   104568594             1    12170.0     11394.0          23564.0\n",
      "3   1060696C4             1    15148.0     14877.0          30025.0\n",
      "4    10CA0004             1     1060.0       527.0           1587.0\n"
     ]
    }
   ],
   "source": [
    "print(patient_community_df.head())\n",
    "patient_community_df.to_csv('patient_projection_afterfalls_without(80,150,152,68).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5216d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=a.creat_patient_svc_interaction_forCommunities(iteration=1)\n",
    "df.to_csv('patient_svc_interaction_HFtable_Hannie_AfterFall_Without(80,150,152,68).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_df = a.df1  # Use df1 from the first iteration for the community data\n",
    "\n",
    "# Count unique patients in each community\n",
    "community_patient_counts = communities_df.groupby('community_id')['patients_id'].nunique().reset_index()\n",
    "community_patient_counts.columns = ['community_id', 'unique_patient_count']\n",
    "\n",
    "# Display the result\n",
    "print(community_patient_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd6222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
