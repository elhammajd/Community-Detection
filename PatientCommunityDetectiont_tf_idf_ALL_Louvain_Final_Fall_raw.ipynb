{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31224b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install python-louvain\n",
    "#! pip install python-igraph\n",
    "#! pip install numpy pandas scikit-learn python-igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f353ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfply import *\n",
    "from IPython.display import display\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import create_engine, event\n",
    "\n",
    "conn_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=KVHSQLPC56;DATABASE=AHDA;Trusted_Connection=yes;'\n",
    "conn_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": conn_str})\n",
    "engine = create_engine(conn_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60038653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of iterations:1\n",
      "Number of all iterations: 1\n",
      "Number of communities in 1 iteration: 3\n",
      "Modularity Value: 0.054287579090583696\n",
      "Enter your desired iterations:1\n",
      "      patients_id  community_id  in_degree  out_degree  weighted_degree\n",
      "0        10001903             1     9843.0     19083.0          28926.0\n",
      "1         1000582             1     6693.0     10041.0          16734.0\n",
      "2       100334194             1    11169.0     21759.0          32928.0\n",
      "3         1003362             1     4484.0      2107.0           6591.0\n",
      "4       1003C3404             1     9334.0     17620.0          26954.0\n",
      "...           ...           ...        ...         ...              ...\n",
      "13382     CC7AB43             3     8965.0     19593.0          28558.0\n",
      "13383     CC93453             3     1344.0      1828.0           3172.0\n",
      "13384     CCB4163             3     4235.0      6036.0          10271.0\n",
      "13385    CCB90234             3     4235.0      6036.0          10271.0\n",
      "13386    CCC9C804             3     1240.0      1648.0           2888.0\n",
      "\n",
      "[13387 rows x 5 columns]\n",
      "Community 1 (Iteration 1): 3349\n",
      "Community 2 (Iteration 1): 5411\n",
      "Community 3 (Iteration 1): 4627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from community import community_louvain\n",
    "from sqlalchemy import create_engine\n",
    "from igraph import Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph\n",
    "\n",
    "class PatientCommunityAnalysis:\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        self.graph2 = None\n",
    "    \n",
    "    def interactions(self):\n",
    "    #add your own query here. For example, for falls, you need to retrieve interactions for all unique patients \n",
    "    #using HF.interactions.\n",
    "        query_dt = ''' \n",
    "            WITH FirstFall AS (\n",
    "       SELECT \n",
    "              Distinct patient_id\n",
    "              ,min([admit_datetime]) as first_fall\n",
    "              ,min(diagnosis_age) as first_fall_age\n",
    "       FROM \n",
    "              Core.acute_care_diagnoses acd\n",
    "       WHERE \n",
    "              diagnosis_type_code NOT IN ('3', '4', '0')\n",
    "              AND SUBSTRING(icd10_code, 1, 3) IN ('W06', 'W07', 'W08', 'W13', 'W14', 'W15', 'W16', 'W17', \n",
    "                                      'X80', 'Y01', 'Y30', 'W00', 'W01', 'W03', 'W04', 'W18', \n",
    "                                      'W10', 'W05')\n",
    "              AND diagnosis_age >= 65\n",
    "       GROUP BY \n",
    "              patient_id\n",
    ")\n",
    "SELECT \n",
    "       i.patient_id, i.service_class_id, i.start_datetime, i.end_datetime\n",
    "FROM \n",
    "       FirstFall\n",
    "INNER JOIN \n",
    "       HF.interactions i ON i.patient_id = FirstFall.patient_id\n",
    "WHERE \n",
    "       i.start_datetime < first_fall\n",
    "AND        \n",
    "        i.service_class_id NOT IN (80,150,152,68)\n",
    "ORDER BY i.patient_id, i.start_datetime\n",
    "        '''\n",
    "        df = pd.read_sql(query_dt, self.engine)\n",
    "        interactions = df[df['service_class_id'].notna()]\n",
    "        #interactions = interactions.dropna() \n",
    "        unique_patients = interactions['patient_id'].unique()\n",
    "       \n",
    "        train_set = interactions[interactions['patient_id'].isin(unique_patients)]\n",
    "        return train_set\n",
    "  \n",
    "    #def sequence(self, interactions_df):\n",
    "     #   df = interactions_df\n",
    "      #  df = df.astype({\"service_class_id\": str}, errors='raise')\n",
    "       # df4 = df.groupby('patient_id')['service_class_id'].agg(' '.join).reset_index()\n",
    "        #return df4\n",
    "  \n",
    "    def patients_projection(self, interactions_df):\n",
    "        df = interactions_df\n",
    "        result_map = defaultdict(dict)\n",
    "        svc_unique = df['service_class_id'].unique()\n",
    "\n",
    "        # Loop over each service and find patients who share the same service\n",
    "        for svc in svc_unique:\n",
    "            index = np.where(df['service_class_id'] == svc)[0]\n",
    "            patients = df.iloc[index]['patient_id'].unique()\n",
    "\n",
    "            for p1 in patients:\n",
    "                for p2 in patients:\n",
    "                    if p1 < p2:  # Avoid double counting\n",
    "                        result_map[p1][p2] = result_map[p1].get(p2, 0) + 1\n",
    "\n",
    "        # Convert the results into a DataFrame with source, target, and weight (shared services)\n",
    "        source, target, weight = [], [], []\n",
    "        for key, neighbors in result_map.items():\n",
    "            for n, w in neighbors.items():\n",
    "                source.append(key)\n",
    "                target.append(n)\n",
    "                weight.append(w)\n",
    "\n",
    "        df3 = pd.DataFrame({'source': source, 'target': target, 'weight': weight})\n",
    "        return df3\n",
    "\n",
    "\n",
    "    def create_graph(self, interactions_df):\n",
    "        # Create patient projection based on shared services\n",
    "        df = self.patients_projection(interactions_df)\n",
    "\n",
    "        # Extract unique patients from both source and target columns\n",
    "        unique_patients = np.unique(df[['source', 'target']])\n",
    "\n",
    "        # Create a mapping from patient IDs to unique integers\n",
    "        id_map = {pat_id: idx for idx, pat_id in enumerate(unique_patients)}\n",
    "\n",
    "        # Use the id_map to replace patient IDs in source and target with integers\n",
    "        df['source'] = df['source'].map(id_map)\n",
    "        df['target'] = df['target'].map(id_map)\n",
    "\n",
    "        # Extract edges (source, target) and weights for the graph\n",
    "        edges = df[['source', 'target']].values.tolist()\n",
    "        weights = df['weight'].tolist()\n",
    "\n",
    "        # Create an igraph graph using the edges and weights\n",
    "        graph = igraph.Graph(edges=edges, edge_attrs={\"weight\": weights})\n",
    "\n",
    "        # Store the graph and reverse mapping (integer to patient ID)\n",
    "        self.graph2 = graph\n",
    "        self.index_to_patient_id = {idx: pat_id for pat_id, idx in id_map.items()}\n",
    "\n",
    "\n",
    "    def community_detection_on_patients(self, number_of_iterations=-1):\n",
    "        louvain = self.graph2.community_multilevel(weights=self.graph2.es['weight'], return_levels=False)\n",
    "\n",
    "        pi = []\n",
    "        piI = []\n",
    "\n",
    "        for i in range(len(louvain)):\n",
    "            component = louvain[i]\n",
    "\n",
    "            if len(component) > 1:\n",
    "                piI.append(component)\n",
    "\n",
    "        pi.append(piI)\n",
    "        iteration = 1\n",
    "        if number_of_iterations == -1:\n",
    "            while True:\n",
    "                piI1 = []\n",
    "                previous_components = pi[iteration - 1]\n",
    "                for community in pi[iteration - 1]:\n",
    "                    induced_subgraph = self.graph2.subgraph(list(community))\n",
    "                    louvain1 = induced_subgraph.community_multilevel(weights=induced_subgraph.es['weight'], return_levels=False)\n",
    "                    for i in range(len(louvain1)):\n",
    "                        subgraph = louvain1[i]\n",
    "                        lst = [community[maped_vertex] for maped_vertex in subgraph]\n",
    "                        piI1.append(lst)\n",
    "                pi.append(piI1)\n",
    "                current_components = pi[iteration]\n",
    "                if not current_components > previous_components:\n",
    "                    break\n",
    "                else:\n",
    "                    iteration += 1\n",
    "\n",
    "        if number_of_iterations != -1:\n",
    "            while iteration < number_of_iterations:\n",
    "                piI1 = []\n",
    "                previous_components = pi[iteration - 1]\n",
    "                for community in pi[iteration - 1]:\n",
    "                    induced_subgraph = self.graph2.subgraph(list(community))\n",
    "                    louvain1 = induced_subgraph.community_multilevel(weights=induced_subgraph.es['weight'], return_levels=False)\n",
    "                    for i in range(len(louvain1)):\n",
    "                        subgraph = louvain1[i]\n",
    "                        lst = [community[maped_vertex] for maped_vertex in subgraph]\n",
    "                        piI1.append(lst)\n",
    "                pi.append(piI1)\n",
    "                current_components = pi[iteration]\n",
    "                if not current_components > previous_components:\n",
    "                    break\n",
    "                else:\n",
    "                    iteration += 1\n",
    "\n",
    "        print(\"Number of all iterations:\", len(pi))\n",
    "        modularity_list = []\n",
    "        for i, iteration in enumerate(pi):\n",
    "            print(f\"Number of communities in {i + 1} iteration: {len(iteration)}\")\n",
    "\n",
    "            membership_vector = [-1] * self.graph2.vcount()\n",
    "\n",
    "            for community_idx, community in enumerate(iteration):\n",
    "                for vertex in community:\n",
    "                    membership_vector[vertex] = community_idx\n",
    "\n",
    "            modularity_value = self.graph2.modularity(membership_vector)\n",
    "            modularity_list.append(modularity_value)\n",
    "            print(\"Modularity Value:\", modularity_value)\n",
    "\n",
    "        self.modularity_list = modularity_list\n",
    "            \n",
    "        self.pi6 = pi\n",
    "        return self.pi6, self.graph2\n",
    "    \n",
    "    def create_table_for_patients(self, iteration2=-1):\n",
    "        iteration2 = iteration2 if iteration2 != -1 else len(self.pi6)\n",
    "        weight = []\n",
    "        out_degree = []\n",
    "        weightd_degree = []\n",
    "        for j in range(len(self.pi6)):\n",
    "            iteration1 = self.pi6[j]\n",
    "\n",
    "            weightI = []\n",
    "            out_degreeI = list()\n",
    "            weightd_degreeI = []\n",
    "            for i in range(len(iteration1)):\n",
    "                component = iteration1[i]\n",
    "\n",
    "                subgraph = self.graph2.induced_subgraph(component)\n",
    "                total_degree = self.graph2.strength(component, mode='all', loops=True, weights=\"weight\")\n",
    "                weightd_degreeI.append(total_degree)\n",
    "                lst = list(range(len(component)))\n",
    "                in_degree = subgraph.strength(lst, mode='all', loops=True, weights=\"weight\")\n",
    "                weightI.append(in_degree)\n",
    "                lst2 = []\n",
    "                for item1, item2 in zip(total_degree, in_degree):\n",
    "                    item = item1 - item2\n",
    "                    lst2.append(item)\n",
    "                out_degreeI.append(lst2)\n",
    "\n",
    "            weight.append(weightI)\n",
    "            out_degree.append(out_degreeI)\n",
    "            weightd_degree.append(weightd_degreeI)\n",
    "\n",
    "        self.vertices = []\n",
    "        idd = 1\n",
    "        id = []\n",
    "\n",
    "        for component in self.pi6[iteration2 - 1]:\n",
    "            for vertex in component:\n",
    "                patient_id = self.index_to_patient_id[vertex]  \n",
    "                self.vertices.append(patient_id)\n",
    "                id.append(idd)\n",
    "            idd += 1\n",
    "\n",
    "        weightt = []\n",
    "        for component in weight[iteration2 - 1]:\n",
    "            for degree in component:\n",
    "                weightt.append(degree)\n",
    "\n",
    "        out_degree2 = []\n",
    "        for component in out_degree[iteration2 - 1]:\n",
    "            for out_deg in component:\n",
    "                out_degree2.append(out_deg)\n",
    "\n",
    "        weighted_degree2 = []\n",
    "        for component in weightd_degree[iteration2 - 1]:\n",
    "            for weight_deg in component:\n",
    "                weighted_degree2.append(weight_deg)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df.insert(loc=0, column='patients_id', value=self.vertices)\n",
    "        df.insert(loc=1, column='community_id', value=id)\n",
    "        df.insert(loc=2, column='in_degree', value=weightt)\n",
    "        df.insert(loc=3, column='out_degree', value=out_degree2)\n",
    "        df.insert(loc=4, column='weighted_degree', value=weighted_degree2)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def count_nodes_in_communities(self, iteration=-1):\n",
    "        iteration = iteration if iteration != -1 else len(self.pi6)\n",
    "        nodes_counts = {}\n",
    "        for i in range(iteration):\n",
    "            communities = self.pi6[i]\n",
    "            for j, community in enumerate(communities):\n",
    "                community_id = f\"Community {j+1} (Iteration {i+1})\"\n",
    "                nodes_counts[community_id] = len(community)\n",
    "        return nodes_counts\n",
    "\n",
    "\n",
    "\n",
    "# Main script\n",
    "\n",
    "custom_iteration1 = float(input(\"Enter the number of iterations:\"))\n",
    "\n",
    "patient_community_analysis = PatientCommunityAnalysis(engine)\n",
    "interactions_df = patient_community_analysis.interactions()\n",
    "\n",
    "#sequence = patient_community_analysis.sequence(interactions_df)\n",
    "\n",
    "patient_community_analysis.create_graph(interactions_df)\n",
    "\n",
    "communities, graph = patient_community_analysis.community_detection_on_patients(number_of_iterations=custom_iteration1)\n",
    "output_file_path = 'Iteration_Information_Fall.txt'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file.write(\"done!\\n\")\n",
    "    file.write(f\"number of all iterations {custom_iteration1}\\n\")\n",
    "    for iteration, community_list in enumerate(communities, start=1):\n",
    "        file.write(f\"number of communities in {iteration} iteration {len(community_list)}\\n\")\n",
    "\n",
    "custom_iteration2 = int(input(\"Enter your desired iterations:\"))\n",
    "patient_community = patient_community_analysis.create_table_for_patients(iteration2=custom_iteration2)\n",
    "patient_community.to_csv('Patient_Community_Projection_Fall.csv', index=False)\n",
    "print(patient_community)\n",
    "\n",
    "nodes_counts = patient_community_analysis.count_nodes_in_communities(iteration=custom_iteration2)\n",
    "for community_id, node_count in nodes_counts.items():\n",
    "    print(f\"{community_id}: {node_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d68e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d9d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
