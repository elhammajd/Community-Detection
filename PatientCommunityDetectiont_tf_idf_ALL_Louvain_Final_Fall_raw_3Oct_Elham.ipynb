{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31224b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install python-louvain\n",
    "#! pip install python-igraph\n",
    "#! pip install numpy pandas scikit-learn python-igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f353ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfply import *\n",
    "from IPython.display import display\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import create_engine, event\n",
    "\n",
    "conn_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=KVHSQLPC56;DATABASE=AHDA;Trusted_Connection=yes;'\n",
    "conn_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": conn_str})\n",
    "engine = create_engine(conn_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60038653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of iterations:10\n",
      "Number of all iterations: 10\n",
      "Number of communities in 1 iteration: 3\n",
      "Modularity Value: 0.05571521788139422\n",
      "Number of communities in 2 iteration: 12\n",
      "Modularity Value: 0.019215500055243494\n",
      "Number of communities in 3 iteration: 51\n",
      "Modularity Value: 0.008254796335223682\n",
      "Number of communities in 4 iteration: 176\n",
      "Modularity Value: 0.0037738720651128107\n",
      "Number of communities in 5 iteration: 461\n",
      "Modularity Value: 0.0031231915107110364\n",
      "Number of communities in 6 iteration: 724\n",
      "Modularity Value: 0.003060400920705846\n",
      "Number of communities in 7 iteration: 807\n",
      "Modularity Value: 0.003063364338106471\n",
      "Number of communities in 8 iteration: 820\n",
      "Modularity Value: 0.0030645858463925484\n",
      "Number of communities in 9 iteration: 822\n",
      "Modularity Value: 0.003064319613286543\n",
      "Number of communities in 10 iteration: 824\n",
      "Modularity Value: 0.0030645862582909266\n",
      "Enter your desired iterations:1\n",
      "      patients_id  community_id  in_degree  out_degree  weighted_degree\n",
      "0        10001903             1    10446.0     18480.0          28926.0\n",
      "1         1000582             1     7010.0      9724.0          16734.0\n",
      "2       100334194             1    11852.0     21076.0          32928.0\n",
      "3         1003362             1     4594.0      1997.0           6591.0\n",
      "4       1003C3404             1     9655.0     17299.0          26954.0\n",
      "...           ...           ...        ...         ...              ...\n",
      "13382     CC70183             3     4623.0      8678.0          13301.0\n",
      "13383    CC7225C4             3      118.0       252.0            370.0\n",
      "13384     CC7AB43             3     6355.0     22203.0          28558.0\n",
      "13385     CCB4163             3     3709.0      6562.0          10271.0\n",
      "13386    CCB90234             3     3709.0      6562.0          10271.0\n",
      "\n",
      "[13387 rows x 5 columns]\n",
      "Community 1 (Iteration 1): 3685\n",
      "Community 2 (Iteration 1): 5878\n",
      "Community 3 (Iteration 1): 3824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from community import community_louvain\n",
    "from sqlalchemy import create_engine\n",
    "from igraph import Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph\n",
    "\n",
    "class PatientCommunityAnalysis:\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        self.graph2 = None\n",
    "    \n",
    "    def interactions(self):\n",
    "    #add your own query here. For example, for falls, you need to retrieve interactions for all unique patients \n",
    "    #using HF.interactions.\n",
    "        query_dt = ''' \n",
    "            WITH FirstFall AS (\n",
    "       SELECT \n",
    "              Distinct patient_id\n",
    "              ,min([admit_datetime]) as first_fall\n",
    "              ,min(diagnosis_age) as first_fall_age\n",
    "       FROM \n",
    "              Core.acute_care_diagnoses acd\n",
    "       WHERE \n",
    "              diagnosis_type_code NOT IN ('3', '4', '0')\n",
    "              AND SUBSTRING(icd10_code, 1, 3) IN ('W06', 'W07', 'W08', 'W13', 'W14', 'W15', 'W16', 'W17', \n",
    "                                      'X80', 'Y01', 'Y30', 'W00', 'W01', 'W03', 'W04', 'W18', \n",
    "                                      'W10', 'W05')\n",
    "              AND diagnosis_age >= 65\n",
    "       GROUP BY \n",
    "              patient_id\n",
    ")\n",
    "SELECT \n",
    "       i.patient_id, i.service_class_id, i.start_datetime, i.end_datetime\n",
    "FROM \n",
    "       FirstFall\n",
    "INNER JOIN \n",
    "       HF.interactions i ON i.patient_id = FirstFall.patient_id\n",
    "WHERE \n",
    "       i.start_datetime < first_fall\n",
    "AND        \n",
    "        i.service_class_id NOT IN (80,150,152,68)\n",
    "ORDER BY i.patient_id, i.start_datetime\n",
    "        '''\n",
    "        df = pd.read_sql(query_dt, self.engine)\n",
    "        interactions = df[df['service_class_id'].notna()]\n",
    "        #interactions = interactions.dropna() \n",
    "        unique_patients = interactions['patient_id'].unique()\n",
    "       \n",
    "        train_set = interactions[interactions['patient_id'].isin(unique_patients)]\n",
    "        return train_set\n",
    "  \n",
    "    #def sequence(self, interactions_df):\n",
    "     #   df = interactions_df\n",
    "      #  df = df.astype({\"service_class_id\": str}, errors='raise')\n",
    "       # df4 = df.groupby('patient_id')['service_class_id'].agg(' '.join).reset_index()\n",
    "        #return df4\n",
    "  \n",
    "    def patients_projection(self, interactions_df):\n",
    "        df = interactions_df\n",
    "        result_map = defaultdict(dict)\n",
    "        svc_unique = df['service_class_id'].unique()\n",
    "\n",
    "        # Loop over each service and find patients who share the same service\n",
    "        for svc in svc_unique:\n",
    "            index = np.where(df['service_class_id'] == svc)[0]\n",
    "            patients = df.iloc[index]['patient_id'].unique()\n",
    "\n",
    "            for p1 in patients:\n",
    "                for p2 in patients:\n",
    "                    if p1 < p2:  # Avoid double counting\n",
    "                        result_map[p1][p2] = result_map[p1].get(p2, 0) + 1\n",
    "\n",
    "        # Convert the results into a DataFrame with source, target, and weight (shared services)\n",
    "        source, target, weight = [], [], []\n",
    "        for key, neighbors in result_map.items():\n",
    "            for n, w in neighbors.items():\n",
    "                source.append(key)\n",
    "                target.append(n)\n",
    "                weight.append(w)\n",
    "\n",
    "        df3 = pd.DataFrame({'source': source, 'target': target, 'weight': weight})\n",
    "        return df3\n",
    "\n",
    "\n",
    "    def create_graph(self, interactions_df):\n",
    "        # Create patient projection based on shared services\n",
    "        df = self.patients_projection(interactions_df)\n",
    "\n",
    "        # Extract unique patients from both source and target columns\n",
    "        unique_patients = np.unique(df[['source', 'target']])\n",
    "\n",
    "        # Create a mapping from patient IDs to unique integers\n",
    "        id_map = {pat_id: idx for idx, pat_id in enumerate(unique_patients)}\n",
    "\n",
    "        # Use the id_map to replace patient IDs in source and target with integers\n",
    "        df['source'] = df['source'].map(id_map)\n",
    "        df['target'] = df['target'].map(id_map)\n",
    "\n",
    "        # Extract edges (source, target) and weights for the graph\n",
    "        edges = df[['source', 'target']].values.tolist()\n",
    "        weights = df['weight'].tolist()\n",
    "\n",
    "        # Create an igraph graph using the edges and weights\n",
    "        graph = igraph.Graph(edges=edges, edge_attrs={\"weight\": weights})\n",
    "\n",
    "        # Store the graph and reverse mapping (integer to patient ID)\n",
    "        self.graph2 = graph\n",
    "        self.index_to_patient_id = {idx: pat_id for pat_id, idx in id_map.items()}\n",
    "\n",
    "\n",
    "    def community_detection_on_patients(self, number_of_iterations=-1):\n",
    "        louvain = self.graph2.community_multilevel(weights=self.graph2.es['weight'], return_levels=False)\n",
    "\n",
    "        pi = []\n",
    "        piI = []\n",
    "\n",
    "        for i in range(len(louvain)):\n",
    "            component = louvain[i]\n",
    "\n",
    "            if len(component) > 1:\n",
    "                piI.append(component)\n",
    "\n",
    "        pi.append(piI)\n",
    "        iteration = 1\n",
    "        if number_of_iterations == -1:\n",
    "            while True:\n",
    "                piI1 = []\n",
    "                previous_components = pi[iteration - 1]\n",
    "                for community in pi[iteration - 1]:\n",
    "                    induced_subgraph = self.graph2.subgraph(list(community))\n",
    "                    louvain1 = induced_subgraph.community_multilevel(weights=induced_subgraph.es['weight'], return_levels=False)\n",
    "                    for i in range(len(louvain1)):\n",
    "                        subgraph = louvain1[i]\n",
    "                        lst = [community[maped_vertex] for maped_vertex in subgraph]\n",
    "                        piI1.append(lst)\n",
    "                pi.append(piI1)\n",
    "                current_components = pi[iteration]\n",
    "                if not current_components > previous_components:\n",
    "                    break\n",
    "                else:\n",
    "                    iteration += 1\n",
    "\n",
    "        if number_of_iterations != -1:\n",
    "            while iteration < number_of_iterations:\n",
    "                piI1 = []\n",
    "                previous_components = pi[iteration - 1]\n",
    "                for community in pi[iteration - 1]:\n",
    "                    induced_subgraph = self.graph2.subgraph(list(community))\n",
    "                    louvain1 = induced_subgraph.community_multilevel(weights=induced_subgraph.es['weight'], return_levels=False)\n",
    "                    for i in range(len(louvain1)):\n",
    "                        subgraph = louvain1[i]\n",
    "                        lst = [community[maped_vertex] for maped_vertex in subgraph]\n",
    "                        piI1.append(lst)\n",
    "                pi.append(piI1)\n",
    "                current_components = pi[iteration]\n",
    "                if not current_components > previous_components:\n",
    "                    break\n",
    "                else:\n",
    "                    iteration += 1\n",
    "\n",
    "        print(\"Number of all iterations:\", len(pi))\n",
    "        modularity_list = []\n",
    "        for i, iteration in enumerate(pi):\n",
    "            print(f\"Number of communities in {i + 1} iteration: {len(iteration)}\")\n",
    "\n",
    "            membership_vector = [-1] * self.graph2.vcount()\n",
    "\n",
    "            for community_idx, community in enumerate(iteration):\n",
    "                for vertex in community:\n",
    "                    membership_vector[vertex] = community_idx\n",
    "\n",
    "            modularity_value = self.graph2.modularity(membership_vector)\n",
    "            modularity_list.append(modularity_value)\n",
    "            print(\"Modularity Value:\", modularity_value)\n",
    "\n",
    "        self.modularity_list = modularity_list\n",
    "            \n",
    "        self.pi6 = pi\n",
    "        return self.pi6, self.graph2\n",
    "    \n",
    "    def create_table_for_patients(self, iteration2=-1):\n",
    "        if iteration2 is None:\n",
    "                iteration_index = len(self.pi6) - 1\n",
    "                print(f\"Iteration not specified, using the last iteration: {len(self.pi6)}\")\n",
    "        else:\n",
    "            iteration_index = iteration2 - 1  \n",
    "\n",
    "        if iteration_index < 0 or iteration_index >= len(self.pi6):\n",
    "            raise ValueError(f\"Invalid iteration index: {iteration2}. Please enter a number between 1 and {len(self.pi6)}.\")\n",
    "\n",
    "        selected_iteration = self.pi6[iteration_index]\n",
    "\n",
    "        self.vertices = []  \n",
    "        community_ids = []  \n",
    "        weightt = []        \n",
    "        out_degree2 = []    \n",
    "        weighted_degree2 = []  #\n",
    "\n",
    "\n",
    "        for community_id, component in enumerate(selected_iteration, start=1):\n",
    "\n",
    "            subgraph = self.graph2.induced_subgraph(component)\n",
    "\n",
    "            patient_ids = [self.index_to_patient_id[vertex] for vertex in component]\n",
    "\n",
    "            in_degree = subgraph.strength(list(range(len(component))), mode='all', loops=True, weights=\"weight\")\n",
    "            total_degree = self.graph2.strength(component, mode='all', loops=True, weights=\"weight\")\n",
    "            out_degree = [total - in_deg for total, in_deg in zip(total_degree, in_degree)]\n",
    "\n",
    "            self.vertices.extend(patient_ids)\n",
    "            community_ids.extend([community_id] * len(component))\n",
    "            weightt.extend(in_degree)\n",
    "            out_degree2.extend(out_degree)\n",
    "            weighted_degree2.extend(total_degree)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'patients_id': self.vertices,\n",
    "            'community_id': community_ids,\n",
    "            'in_degree': weightt,\n",
    "            'out_degree': out_degree2,\n",
    "            'weighted_degree': weighted_degree2\n",
    "        })\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def count_nodes_in_communities(self, iteration=-1):\n",
    "        iteration = iteration if iteration != -1 else len(self.pi6)\n",
    "        nodes_counts = {}\n",
    "        for i in range(iteration):\n",
    "            communities = self.pi6[i]\n",
    "            for j, community in enumerate(communities):\n",
    "                community_id = f\"Community {j+1} (Iteration {i+1})\"\n",
    "                nodes_counts[community_id] = len(community)\n",
    "        return nodes_counts\n",
    "\n",
    "\n",
    "custom_iteration1 = float(input(\"Enter the number of iterations:\"))\n",
    "\n",
    "patient_community_analysis = PatientCommunityAnalysis(engine)\n",
    "interactions_df = patient_community_analysis.interactions()\n",
    "\n",
    "#sequence = patient_community_analysis.sequence(interactions_df)\n",
    "\n",
    "patient_community_analysis.create_graph(interactions_df)\n",
    "\n",
    "communities, graph = patient_community_analysis.community_detection_on_patients(number_of_iterations=custom_iteration1)\n",
    "output_file_path = 'Iteration_Information_Fall_FifthRun.txt'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file.write(\"done!\\n\")\n",
    "    file.write(f\"number of all iterations {custom_iteration1}\\n\")\n",
    "    for iteration, community_list in enumerate(communities, start=1):\n",
    "        file.write(f\"number of communities in {iteration} iteration {len(community_list)}\\n\")\n",
    "\n",
    "custom_iteration2 = int(input(\"Enter your desired iterations:\"))\n",
    "patient_community = patient_community_analysis.create_table_for_patients(iteration2=custom_iteration2)\n",
    "patient_community.to_csv('Elham_Patient_Community_Projection_Fall_FifthRun.csv', index=False)\n",
    "print(patient_community)\n",
    "\n",
    "nodes_counts = patient_community_analysis.count_nodes_in_communities(iteration=custom_iteration2)\n",
    "for community_id, node_count in nodes_counts.items():\n",
    "    print(f\"{community_id}: {node_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d68e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class CommunityAnalysisReport:\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        #self.cohort = None  \n",
    "        \n",
    "    def take_svcid_HF(self):\n",
    "        query_s1 = \"\"\" \n",
    "            SELECT DISTINCT service_class_id \n",
    "            FROM HF.interactions \n",
    "            WHERE service_class_id != -1\n",
    "            AND mhsu_flag = 1\n",
    "            AND service_class_id NOT IN (17, 18)\n",
    "            AND start_datetime >= '2016-04-01'\n",
    "            GROUP BY service_class_id\n",
    "            \"\"\"\n",
    "        return pd.read_sql(query_s1, self.engine)\n",
    "        \n",
    "    def interactionsAndPatientsOFeachSvc_HF(self, df):\n",
    "        patient_string = '\\',\\''.join(df[\"patients_id\"])\n",
    "        patient_string = '\\'' + patient_string + '\\''\n",
    "\n",
    "        query_s2 = \"\"\" \n",
    "            SELECT service_class_id, COUNT(DISTINCT patient_id) AS patients\n",
    "            FROM HF.interactions \n",
    "            WHERE patient_id IN (\"\"\" + patient_string + \"\"\") \n",
    "            AND service_class_id != -1\n",
    "            AND age_at_encounter BETWEEN 5 AND 21\n",
    "            AND mhsu_flag = 1\n",
    "            AND service_class_id NOT IN (17, 18)\n",
    "            AND patient_id NOT IN ('177CA9AA4', '2991143')\n",
    "            AND start_datetime >= '2016-04-01'\n",
    "            GROUP BY service_class_id\n",
    "            ORDER BY patients DESC \n",
    "            \"\"\"\n",
    "\n",
    "        return pd.read_sql(query_s2, self.engine)\n",
    "    \n",
    "    def safe_division1(self,x, y):\n",
    "        if y == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return round(((x / y)*100), 2)\n",
    "\n",
    "    def interactions_and_patients_of_each_svc_HF(self, df, iteration):\n",
    "        \n",
    "        take_svcid_label = self.take_svcid_HF()\n",
    "        i = 0\n",
    "        for community in df.community_id.unique():\n",
    "            index=np.where(df.community_id==community)\n",
    "            community_df=df.iloc[index]\n",
    "            interactionsAndPatientsOFeachSvc_df= self.interactionsAndPatientsOFeachSvc_HF(community_df)          \n",
    "            interactionsAndPatientsOFeachSvcMergeWithAllSvc= pd.merge(take_svcid_label, interactionsAndPatientsOFeachSvc_df, on='service_class_id', how='left')\n",
    "            new_col_name = str(community)+ '  #patients'\n",
    "            new_col_name2 = str(community)+ '  #interactions' \n",
    "            col_dict = {'patients':new_col_name }\n",
    "            interactionsAndPatientsOFeachSvcMergeWithAllSvc=interactionsAndPatientsOFeachSvcMergeWithAllSvc.rename(columns=col_dict)\n",
    "            interactionsAndPatientsOFeachSvcMergeWithAllSvc=interactionsAndPatientsOFeachSvcMergeWithAllSvc.fillna(0)\n",
    "\n",
    "            summury=interactionsAndPatientsOFeachSvcMergeWithAllSvc\n",
    "            summury['community%'+ str(community)] =  summury.apply(lambda row: self.safe_division1(row.iloc[1], len(community_df)), axis=1)\n",
    "            if(i!=0):\n",
    "                df_copy=pd.merge( df_copy,  summury, on='service_class_id', how='left')\n",
    "\n",
    "            if(i==0):  \n",
    "                df_copy =  summury.copy(deep=True)\n",
    "            i=i+1\n",
    "        result_df_s = df_copy.fillna(0)\n",
    "        \n",
    "        query = \"\"\"\n",
    "                SELECT DISTINCT i.service_class_id, s.service_class_name\n",
    "                FROM HF.interactions i\n",
    "                JOIN Ref.service_classifications s ON i.service_class_id = s.service_class_id;\n",
    "                \"\"\"\n",
    "        df_service = pd.read_sql(query, self.engine)\n",
    "        \n",
    "        for service_class_id in result_df_s['service_class_id'].unique():\n",
    "            service_name = df_service.loc[df_service['service_class_id'] == service_class_id, 'service_class_name'].iloc[0]\n",
    "            result_df_s.loc[result_df_s['service_class_id'] == service_class_id, 'service_class_name'] = service_name\n",
    "        columns = result_df_s.columns.tolist()\n",
    "\n",
    "        columns.insert(1, columns.pop())\n",
    "        result_df_s = result_df_s[columns]\n",
    "       \n",
    "        return result_df_s\n",
    "    \n",
    "\n",
    "community_report = CommunityAnalysisReport(engine)\n",
    "\n",
    "custom_iteration3 = int(input(\"Enter your desired iterations:\"))\n",
    "\n",
    "df = patient_community_analysis.create_table_for_patients(iteration2=custom_iteration3)\n",
    "\n",
    "service = community_report.interactions_and_patients_of_each_svc_HF(df, iteration=custom_iteration3)\n",
    "service.to_csv(f'Elham_Community_Service_Summary_Result_FifthRun.csv', index=False)  \n",
    "print(service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d9d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
